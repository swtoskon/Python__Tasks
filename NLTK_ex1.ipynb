{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/Users/swtoskon/Downloads/exercises/viruses-12-00135-v2.txt','r')\n",
    "raw=f.read()\n",
    "tokens = nltk.word_tokenize(raw,\"english\")\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens is  3914\n"
     ]
    }
   ],
   "source": [
    "word_count = len(tokens)\n",
    "print(\"The number of tokens is \",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The types of the article is  1134\n"
     ]
    }
   ],
   "source": [
    "vocabulary = len(set(tokens))\n",
    "print(\"The types of the article is \",vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXCLUDE  PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3524\n",
      "1118\n"
     ]
    }
   ],
   "source": [
    "tokens2 = nltk.regexp_tokenize(raw, '\\w+')\n",
    "print(len(tokens2))\n",
    "print(len(set(tokens2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUMBER OF SENTENCES CONTAINED IN THE ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences contained in the article is  148\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(raw)\n",
    "number_of_sentences = len(sentences)\n",
    "print(\"The number of sentences contained in the article is \" , number_of_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEXICAL DIVERSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lexical Diversity of this article is 0.2897291773122126\n"
     ]
    }
   ],
   "source": [
    "lexical_diversity = vocabulary/word_count\n",
    "print(\"The lexical Diversity of this article is\",lexical_diversity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lexical Diversity of this article is 3.451499118165785\n"
     ]
    }
   ],
   "source": [
    "lexical_diversity2 = word_count/vocabulary\n",
    "print(\"The lexical Diversity of this article is\",lexical_diversity2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 most common lexical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 620), ('JJ', 447), ('IN', 442), ('DT', 344), ('NNP', 306)]\n"
     ]
    }
   ],
   "source": [
    "tagged_fd = nltk.FreqDist(tag for (word, tag) in tagged)\n",
    "print(tagged_fd.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 most common words without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop_words=stopwords.words(\"english\") + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sent=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = nltk.Text(filtered_sent)\n",
    "fdist = nltk.FreqDist(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2019-nCoV', 56), ('human', 32), ('cases', 30), ('SARS-CoV', 24), ('MERS-CoV', 21), ('outbreak', 19), ('CoV', 18), ('virus', 18), ('Wuhan', 16), ('patients', 16)]\n"
     ]
    }
   ],
   "source": [
    "ten_most_common_words = fdist.most_common(10)\n",
    "print(ten_most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('human', 'human'), 10), (('N', 'protein'), 7), (('super', 'spreading'), 7), (('group', '2B'), 6), (('novel', 'CoV'), 6), (('around', 'world'), 6), (('human', 'ACE2'), 6), (('exported', 'cases'), 5), (('health', 'care'), 5), (('novel', 'coronavirus'), 4)]\n"
     ]
    }
   ],
   "source": [
    "bigrams   = nltk.FreqDist(nltk.bigrams(filtered_sent))\n",
    "ten_most_common_bigrams = bigrams.most_common(10)\n",
    "print(ten_most_common_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total nouns in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, tag) in tagged:\n",
    "         counts[tag] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged2 = nltk.pos_tag(text,tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n"
     ]
    }
   ],
   "source": [
    "total_nouns = counts[\"NN\"] +counts[\"NNP\"]+counts[\"NNPS\"]+counts[\"NNS\"]\n",
    "print(total_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts2 = collections.defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, tag) in tagged2:\n",
    "         counts2[tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n"
     ]
    }
   ],
   "source": [
    "print(counts2['NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
